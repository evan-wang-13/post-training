{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ewang\\Documents\\Cornell_Research\\DPO\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "def load_model_and_tokenizer(\n",
    "    base_model_id: str = \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    adapter_path: str = \"./qwen_sft_final\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Load the base model and merge it with the trained LoRA weights\n",
    "    \"\"\"\n",
    "    # Load base model with same quantization as training\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load and apply the LoRA adapter\n",
    "    model = PeftModel.from_pretrained(model, adapter_path)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_response(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a response from the model\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting generation...\n",
      "\n",
      "Prompt: Explain the concept of machine learning to a 5 year old.\n",
      "\n",
      "Response: Explain the concept of machine learning to a 5 year old. Imagine you have a toy car and you want to make it faster by training it with new tricks or algorithms. The toy car is your data, which is like the \"training set\" or \"input\". You will use this data to create rules that help it learn from its mistakes.\n",
      "\n",
      "Machine learning is like using a magic wand: It looks at how the toy car learns to drive on a track (the \"problem\"). By applying a bunch of different rules (what the algorithm does), the toy car can get better and better, just like you can learn things as you go along!\n",
      "\n",
      "Imagine we're talking about building our own toy car. A toy car has wheels (like the \"output\") and a battery (like the \"train signals\"). We want to train our toy car so it knows how to turn around when it needs to stop, and how to keep moving forward for longer periods.\n",
      "\n",
      "We use a special tool called an algorithm (or model) to do all the work! The algorithm takes in the current position of the toy car (called the \"input\"), then applies one rule after another until it gets the correct answer. When the toy car makes the right decision, it's done! And if there's any wrong decision, the algorithm will show us where it went\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt: Write a short poem about artificial intelligence.\n",
      "\n",
      "Response: Write a short poem about artificial intelligence. Artificial intelligence is a term used to describe any machine that can perform tasks or solve problems using algorithms and data analysis. It has the potential to revolutionize industries, but it also raises concerns about job loss and privacy.\n",
      "\n",
      "In a world where machines are constantly learning from data, AI could potentially replace many jobs, especially in sectors such as healthcare, finance, and transportation. This could lead to economic instability and social disruption.\n",
      "\n",
      "However, with such advancements come ethical considerations. What happens if an AI system becomes too good at solving complex problems? Will people trust it enough to rely on it for every decision they make? How would we ensure its use remains fair and inclusive?\n",
      "\n",
      "Moreover, how do we address the issue of privacy when AI systems are being trained on personal data? What safeguards can be put in place to protect individuals' privacy and prevent misuse?\n",
      "\n",
      "Despite these challenges, it's clear that AI holds immense promise. As technology continues to advance, so will our understanding of what it means to live in an interconnected and digital world. So, let's embrace this opportunity while ensuring that the benefits of AI are shared equitably among all members of society. #ArtificialIntelligence #AIPrivacy #DataSafety\n",
      "\n",
      "How would you suggest addressing the concern about privacy in AI-based solutions, particularly in\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load model and tokenizer\n",
    "    model, tokenizer = load_model_and_tokenizer()\n",
    "    \n",
    "    # Test prompts\n",
    "    test_prompts = [\n",
    "        \"Explain the concept of machine learning to a 5 year old.\",\n",
    "        \"Write a short poem about artificial intelligence.\",\n",
    "        # Add more test prompts here\n",
    "    ]\n",
    "    \n",
    "    print(\"Starting generation...\")\n",
    "    for prompt in test_prompts:\n",
    "        print(\"\\nPrompt:\", prompt)\n",
    "        print(\"\\nResponse:\", generate_response(model, tokenizer, prompt))\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 1\n",
      "Current GPU: NVIDIA GeForce RTX 2060 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [01:53<00:00, 56.64s/it] \n",
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with a simple prompt...\n",
      "\n",
      "Response from model:\n",
      "Write a hello world program in Python.\n",
      "\n",
      "```python\n",
      "print(\"Hello world!\")\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* `print()` is a built-in Python function that prints the given argument to the console.\n",
      "* `\"Hello world!\"` is the string that we want to print.\n",
      "* `` is the string delimiter, which tells `print()` to print the argument on a single line.\n",
      "\n",
      "**Output:**\n",
      "\n",
      "```\n",
      "Hello world!\n",
      "```\n",
      "\n",
      "**Note:**\n",
      "\n",
      "* The `print()` function can take multiple arguments, which will be separated by commas.\n",
      "* You can use different formatting options\n",
      "\n",
      "Success! You have access to Gemma!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import huggingface_hub\n",
    "\n",
    "\n",
    "\n",
    "def test_gemma_access():\n",
    "    try:\n",
    "        print(\"Loading tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "        \n",
    "        print(\"Loading model...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\")\n",
    "        \n",
    "        print(\"Testing with a simple prompt...\")\n",
    "        prompt = \"Write a hello world program in Python\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        outputs = model.generate(**inputs, max_length=128)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(\"\\nResponse from model:\")\n",
    "        print(response)\n",
    "        \n",
    "        print(\"\\nSuccess! You have access to Gemma!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError occurred: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_gemma_access()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
